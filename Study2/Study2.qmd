---
title: "Predicting Health Outcomes Using NHANES Data: Insights from Multivariable Analyses"
author: "Bosco Bakwatanisa"
date: last-modified
format: 
 html:
    fig-responsive: true
    fig-align: center
    fig-width: 8
    fig-height: 6
    page-layout: full
    embed-resources: true
    number_sections: TRUE
    date-format: iso
    code-fold: false
    toc: true
    toc-depth: 3
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

# 1. Setup and Data Ingest

## 1.1. Loading R packages

```{r echo=TRUE, warning=FALSE, message=FALSE,devtools::install_github("cjendres1/nhanes")}
#load required R libraries
library(nhanesA)
library(forcats)
library(boot)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(janitor)
library(naniar)
library(psych)
library(patchwork)
library(gridExtra)
library(mice)
library(broom.mixed)
library(Epi)
library(vcd)
library(xfun)
library(tidyverse)
theme_set(theme_light())  
knitr::opts_chunk$set(comment=NA)
```

# 2. Cleaning the Data

## 2.1. Identify a Quantitative outcome

The qualitative outcome of this analysis is Body Mass Index (BMI). BMI is often used as an indicator for assessing body fat based on height and weight and is also quiet associated with an individual's quality of life.BMI data to be used for this analysis was corrected from participants from the 2017-March 2020 NHANES survey and was recorded as BMXBMI. \## 2.2. Identify key predictor and three other predictors of outcome

The key predictor for this analysis is Gender. Quite often gender physiological differences between people of different sex prevail and these may result in differences in BMI due to biological, hormonal, and societal factors influencing body composition, metabolism, lifestyle behaviors among others. Understanding how BMI varies with gender within a population and how this relationship may be affected by other cofounding factors including age, smoking status, and race may provide us with insights on how this association may influence the gender based health outcomes within the United States population.

## 2.3. Data preparation

Data is obtained from the 2017-March 2020 survey data through the nhanesA package, This package allows you to access data from demographics data, experimental data, Questionnaires among others.How we gather all the data used on the analysis is shown in the code section below.

<!---```{r echo=TRUE, warning=FALSE, message=FALSE, }

BMX_raw <- nhanes("P_BMX")|> tibble() 
saveRDS(BMX_raw, file = "data/P_BMX.rds")
demo_raw <- nhanes('P_DEMO') |> tibble()
saveRDS(demo_raw, file = "data/P_DEMO.rds")
SMQ_raw <- nhanes("P_SMQ")|> tibble()
saveRDS(SMQ_raw, file = 'data/SMQ.rds')
demo_raw <- readRDS("data/P_DEMO.rds")
BMX_raw<- readRDS("data/P_BMX.rds")
DEMO <- demo_raw %>%select(SEQN,RIDSTATR,RIAGENDR,RIDAGEYR,RIDRETH3 )

BMXBMI <- BMX_raw%>%select(SEQN,BMXBMI)
analysis_tibble<- left_join(DEMO, BMXBMI, by = "SEQN")
SMQ_raw <- readRDS("data/SMQ.rds")
smoke_data <- SMQ_raw %>% select(SEQN,SMQ020)
Analysis2Data <- left_join(analysis_tibble, smoke_data, by = "SEQN")

analysis2Data <- Analysis2Data %>% filter( RIDAGEYR >=21 &  RIDAGEYR<= 79) %>%
  rename(BMI = BMXBMI, Age = RIDAGEYR, Gender =RIAGENDR, Race = RIDRETH3, Smoking = SMQ020) %>%
  mutate(Race = factor(
    Race,
    levels = c("Mexican American","Other Hispanic","Non-Hispanic White",    "Non-Hispanic Black","Non-Hispanic Asian"),
   labels = c("Mexican","Hispanic", "White","Black", "Asian")),Gender = ifelse(Gender == "Male", 1,2))%>%filter(BMI > 0) %>% filter(Smoking %in% c('Yes', 'No')) %>%  mutate(
    Smoking = ifelse(Smoking =="Yes","Smoker","Non_Smoker")) %>% select(SEQN, Age, Gender, Race, Smoking, BMI)%>% mutate(Gender = factor(
      Gender, levels = c(1,2),labels =c("Male","Female")),Smoking = factor(Smoking,levels = c("Smoker","Non_Smoker"), labels =c("Smoker","Non_Smoker")
    ))

saveRDS(analysis2Data, file = 'data/Analyses2Data.rds')

```
--->

```{r echo=TRUE, warning=FALSE, message=FALSE}
analysis2Data <- readRDS('data/Analyses2Data.rds')
# Check for missing values
missing_summary <- colSums(is.na(analysis2Data))
missing_summary

```

```{r echo=TRUE, warning=FALSE, message=FALSE}
# Imputation using `mice`
imputed_data <- mice(analysis2Data, m = 1, method = 'pmm', maxit = 5)
complete_data <- complete(imputed_data)

# Display summary
kable(head(complete_data)) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

# 3. Codebook and Data Description

```{r echo=TRUE, warning=FALSE, message=FALSE}
variable_description <- tibble::tribble(
  ~Variable_Name, ~Role_in_Analysis,~Type, ~Original_Code,
  "SEQN",  "ID",   "Quant",   " Respondent sequence number",
  "RIDSTATR","Interview/Examination status","Binary","RIDSTATR",
  "Gender",  "Analysis Key Predictor", "Binary", "RIAGENDR",
  "Age", "Analysis key predictor and used to select Adult persons aged 21-79 years","Quant","RIDAGEYR",
  "Race/Ethnicity", "Analysis Predictor",  "X-cat", "RIDRETH3 ","BMI","Analysis Outocome", "Quant", "BMXBMI",
  "Smoking", "Analysis Predictor",  "Binary", "SMQ020")
variable_description |> kbl() |> kable_styling(bootstrap_options = "striped", full_width = F)


```

### 3.1. Data Description

```{r echo=TRUE, warning=FALSE, message=FALSE}
analysis2Data <- readRDS('data/Analyses2Data.rds')
analysis2Data%>% select(Age, Gender, Race, Smoking, BMI) %>% describe() %>% kable()%>% kable_styling(bootstrap_options = "striped", full_width = F)

```

The participants included in our analysis had a mean age of 49 and median age of 50, with mean Body Mass Index (BMI) of 30.24 and median BMI of 28.9, gender was categorized into female and males, race had 5 levels of Mexican American, Non-Hispanic White, Non-Hispanic black, Non-Hispanic Asian, and other Hispanics.

# 4. My Research Question

### **4.1. Problem Statement**

Body Mass Index (BMI) is a widely used indicator for assessing body fat based on height and weight. Gender differences in BMI may arise due to biological, hormonal, and societal factors influencing body composition, metabolism, lifestyle behaviors among others. Understanding how BMI varies with gender within a population while taking into account other confounding factors such as age, race/ethnicity, and smoking status may provide us with insights on how this association may influence the gender based health outcomes within the United States population.

### 4.2. Research Question

Is there significant relationship between Body Mass Index (BMI) and gender, while adjusting for an individual's age, race/ethnicity, and smoking status?

# 5. Partitioning the Data

Here, I divided my analysis data into training and testing data sets in a split ratio of 70% to 30%. This provides us with an opportunity to validate our model by prediction with the test dataset.

```{r echo=TRUE, warning=FALSE, message=FALSE}
# Split analysis data into training (70%) and testing (30%)
set.seed(112724)
training_data <- complete_data %>% slice_sample(prop = 0.7)
testing_data <- anti_join(complete_data, training_data, by = "SEQN")

# Confirm split
nrow(training_data)
nrow(testing_data)
nrow(training_data) + nrow(testing_data) == nrow(complete_data) # Should return TRUE


```

The data was split into the training data set with 5412 participants and a testing data set with 2320 participants.

# 6. Transforming the Outcome

Using the histogram and normal Q-Q plots, I assessed the normality and the data distribution of the outcome variable to provide an understanding of whether I needed to perfomr any data transformations.

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Histogram and Q-Q Plot
p1 <- ggplot(training_data, aes(x = BMI)) +
  geom_histogram(bins = 50,binwidth = 3, fill = "blue", alpha = 0.7) +
  labs(title = "BMI Distribution")+
  theme_classic(base_size = 8) +
  theme(legend.position = "left",
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

p2 <- ggplot(training_data, aes(sample = BMI)) +
  geom_qq(col = "slateblue") + 
  geom_qq_line(col = "magenta") + 
  theme(aspect.ratio = 1) + 
  labs(title = "Normal Q-Q: NHANES BMI")+
  theme_classic(base_size = 8) +
  theme(legend.position = "none",
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))
p1 + p2


```

Both the histogram and the normal Q-Q plot, I noticed that the BMI distribution violated the normality assumption as seen from both plots that the data is right skewed and therefore a need to transform the data using perhaps log transform normalization.

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Histogram and Q-Q Plot for log transformed BMI
p1 <- ggplot(training_data, aes(x = log(BMI))) +
  geom_histogram(bin = 50,binwidth = .08, fill = "blue", alpha = 0.7) +
  labs(title = "log(BMI) Distribution")+
  theme_classic(base_size = 8) +
  theme(legend.position = "left",
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

p2 <- ggplot(training_data, aes(sample = log(BMI))) +
  geom_qq(col = "slateblue") + 
  geom_qq_line(col = "magenta") + 
  theme(aspect.ratio = 1) + 
  labs(title = "Normal Q-Q: NHANES log(BMI)")+
  theme_classic(base_size = 8) +
  theme(legend.position = "none",
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))
p1 + p2
```

After the log transformation, the outcome variable more symmetrical and more most of the points lied on the line, with a little left skewness.

# 7. The Big Model

Using the training data, I fitted a multivariate linear regression model as a means of assessing the association between Gender and log transformed Body mass index and other confounding variables of age, race/ethnicity, and smoking status

```{r echo=TRUE, warning=FALSE, message=FALSE}
# Big Model
training_data <- training_data%>% mutate(log_BMI = log(BMI))
big_model <- lm(log_BMI ~ Gender + Age + Race + Smoking, data = training_data)
summary(big_model)

```

### 7.1. Big Model Summary Statistics

```{r echo=TRUE, warning=FALSE, message=FALSE}
tidy(big_model)%>% kable(digits = 3)%>%kable_styling(bootstrap_options = "striped", full_width = F)
```

From the Big model we observed that being female, age, and race: Hispanic, non-Hispanic white, non-Hispanic black, and non-Hispanic Asian were statistically significant predictors of an individuals BMI (p-value \< 0.05).

### 7.2. Model Residuals

```{r echo=TRUE, warning=FALSE, message=FALSE}
#Extracting the residuals from the model
residuals <- resid(big_model)
fitted_values <- fitted(big_model)
```

### 7.3. Diagnostic Plots

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Residuals vs Fitted Plot
p1<- ggplot(data = data.frame(fitted_values, residuals), aes(x = fitted_values, y = residuals)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted",
       x = "Fitted Values",
       y = "Residuals") +
  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 9),
        axis.title.y = element_text(face = "bold", size = 9),
        axis.title.x = element_text(face = "bold", size = 9),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

# Normal Q-Q Plot
p2 <- ggplot(data = data.frame(residuals), aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Normal Q-Q Plot",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 9),
        axis.title.y = element_text(face = "bold", size = 9),
        axis.title.x = element_text(face = "bold", size = 9),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

# Scale-Location Plot
sqrt_abs_residuals <- sqrt(abs(residuals))
p3 <- ggplot(data = data.frame(fitted_values, sqrt_abs_residuals), aes(x = fitted_values, y = sqrt_abs_residuals)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Scale-Location Plot",
       x = "Fitted Values",
       y = "√|Residuals|") +
    theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 9),
        axis.title.y = element_text(face = "bold", size = 9),
        axis.title.x = element_text(face = "bold", size = 9),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

# Residuals vs Leverage Plot
leverage <- hatvalues(big_model)
p4 <- ggplot(data = data.frame(leverage, residuals), aes(x = leverage, y = residuals)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Leverage",
       x = "Leverage",
       y = "Residuals") +
   theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 9),
        axis.title.y = element_text(face = "bold", size = 9),
        axis.title.x = element_text(face = "bold", size = 9),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 10))

p1 + p2 + p3 + p4
     

```

From the residuals vs. Fitted plot, I observed that the residuals do not appear to be randomly scattered around the red dashed horizontal line at zero and there are clusters indicating non-linearity. The spread of residuals seemed to increase slightly for higher fitted values, suggesting heteroscedasticity. The normal Q-Q plot shows that residuals deviate from the red line, particularly in the top tail, indicating that residuals are not normally distributed, hence suggesting skewness. Additionally, from the Scale-location plot, the red trend line shows a slight upward slope, suggesting increasing variance of residuals with fitted values. This altogether points to violation of assumptions for homoscedasticity, linearity, normality of residuals.

# 8. The Smaller Model

The small model only interrogated the direct relationship between BMI and Gender without considering and confounding variables

```{r echo=TRUE, warning=FALSE, message=FALSE}
# Smaller Model
small_model <- lm(log_BMI ~ Gender, data = training_data)
Summary_small_model <- summary(small_model)
Summary_small_model

```

**Extract Summary stastics from the small model**

```{r echo=TRUE, warning=FALSE, message=FALSE}
tidy(small_model, conf.int = TRUE)%>% kable(digits = 3)%>% kable_styling(bootstrap_options = "striped", full_width = F)


```

# 9. In-Sample Comparison

Here, we assess model performance between the big-model and small-model using the training data set and assess the quality of model fitting by looking at key statistical measures such as R-Squared, Adjusted R-Squared, Akaike Information criterion (AIC) and Bayesian Information Criterion (BIC) and Log-Likelihood

## 9.1. Quality of Fit

```{r echo=TRUE, warning=FALSE, message=FALSE}
big_model_summary <- glance(big_model)
small_model_summary <- glance(small_model)

fit_comparison <- data.frame(
  Model = c("Big Model", "Small Model"),
  R_squared = c(big_model_summary$r.squared, small_model_summary$r.squared),
  Adjusted_R_squared = c(big_model_summary$adj.r.squared, small_model_summary$adj.r.squared),
  AIC = c(big_model_summary$AIC, small_model_summary$AIC),
  BIC = c(big_model_summary$BIC, small_model_summary$BIC)
)
fit_comparison%>% kable(digits = 3)%>%kable_styling(bootstrap_options = "striped", full_width = F)
```

Based on the Adjusted_R_squared, AIC, and BIC, the big_model showed a very good fit to the data given it showed the lowest most negative AIC, BIC values. This implies that together with other confounding factors, Gender better predicts an individual's BMI.

## 9.2. Posterior Predictive Checks

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
training_data$Big_Model_Pred <- predict(big_model, training_data)
training_data$Small_Model_Pred <- predict(small_model, training_data)

ggplot(training_data, aes(x = log(BMI))) +
  geom_density(aes(fill = "Observed"), alpha = 0.5) +
  geom_density(aes(x = Big_Model_Pred, fill = "Big Model Prediction"), alpha = 0.5) +
  geom_density(aes(x = Small_Model_Pred, fill = "Small Model Prediction"), alpha = 0.5) +
  scale_fill_manual(values = c("Observed" = "blue", 
                               "Big Model Prediction" = "red", 
                               "Small Model Prediction" = "green")) +
  labs(title = "Posterior Predictive Check: Observed vs Predicted BMI",
       x = "log_BMI", y = "Density", fill = "Model") +
   theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))
```

From the plot, we observe that the Big Model's predictions align more closely with the observed density curve compared to the Small model. This suggests that the big model captures the variability in the data more effectively by incorporating additional predictors. This particularly reduces prediction error and improves model accuracy.

## 9.3. Assessing Assumptions

To check whether the fitted models meets the linearity assumption, normality of residuals, homoscedasticity, and influential points, I extracted the residuals and their fitted values to plot the 'Residuals vs. Fitted Plot', normality Q-Q plot, Scale-location plot, and the residuals vs. leverage plot as shown below.

### 9.3.1. Residuals vs. Fitted plot

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Residuals for Big and Small Models
Residual_Big <- augment(big_model)
Residual_Small <- augment(small_model)

# Residual plots
p1 <- ggplot(Residual_Big)+ geom_point(aes(x =.fitted, y =.resid),color = "red") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs. Predicted (Big Model)",
       x = "Predicted BMI (Big Model)",
       y = "Residuals") +  theme_classic(base_size = 8) +
theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

p2 <- ggplot(Residual_Small) +
  geom_point(aes(x = .fitted, y = .resid), color = "green") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs. Predicted (Small Model)",
       x = "Predicted BMI (Small Model)",
       y = "Residuals") + theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

p1+p2
```

From the residuals vs. predicted plots above, the Big model shows more evenly distributed residuals around the zero line, suggesting it. captures the variance in the data better and meets the homoscedasticity assumption, whereas on the other hand, the small model has residuals clustered at specific predicted values, indicating poor fit and limited variability in its predictions, reflecting the oversimplification of using only gender as a predictor.

### 9.3.2. Q-Q Plot

This plot assesses normality of residuals by comparing them to a thretical normal distribution.

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Big Model Q-Q Plot
p1 <- ggplot(Residual_Big, aes(sample = .resid)) +
  stat_qq(color = "blue") +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot (Big Model)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

# Small Model Q-Q Plot
 p2 <- ggplot(Residual_Small, aes(sample = .resid)) +
  stat_qq(color = "green") +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot (Small Model)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

 p1+p2
```

From the q-Q plot, we observed that for the small Model, the residuals more closely to the theoretical quantile line,suggesting better adherence to the normality assumption, implying a better fit, whereas the big model show deviation from the theoretical quantile line, particularly at the extremes, suggesting a lack of normality in the residuals, highlighting the presence of potential outliers affecting the distribution. 

### 9.3.3. Scale-Location Plot

Here, we plot the square root of the standardized residuals to assess constant variance

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Big Model Scale-Location Plot
#Residual_Big$Std_Residuals <- sqrt(abs(scale(big_model_residuals$Residuals)))

p1 <- ggplot(Residual_Big, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Scale-Location Plot (Big Model)",
       x = "Fitted Values",
       y = "√|Standardized Residuals|") +  theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

# Small Model Scale-Location Plot
#small_model_residuals$Std_Residuals <- sqrt(abs(scale(small_model_residuals$Residuals)))

p2 <- ggplot(Residual_Small, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = "green", alpha = 0.6) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Scale-Location Plot (Small Model)",
       x = "Fitted Values",
       y = "√|Standardized Residuals|") +  theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))
p1+p2
```

The Scale-Laction plot for the big model(left) indicates heteroscedasticity, as the spread of residuals varies with fitted values, and the red trendline is not flat, suggesting non-constant variance. Conversely, the small model (right) shows a consistent spread of residuals around the red line, indicating more uniform variance but limited variability in fitted values. These results suggest that while the small model has a simpler structure, the Big Model may suffer from issues related to over-fitting. 

### 9.3.4. Residuals vs. Leverage Plot

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}
# Big Model Residuals vs Leverage
big_model_influence <- data.frame(
  Leverage = hatvalues(big_model),
  Residuals = resid(big_model),
  Cooks_Distance = cooks.distance(big_model)
)

p1 <- ggplot(big_model_influence, aes(x = Leverage, y = Residuals, size = Cooks_Distance)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  scale_size_continuous(name = "Cook's Distance") +
  labs(title = "Residuals vs Leverage (Big Model)",
       x = "Leverage",
       y = "Residuals") +  theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

# Small Model Residuals vs Leverage
small_model_influence <- data.frame(
  Leverage = hatvalues(small_model),
  Residuals = resid(small_model),
  Cooks_Distance = cooks.distance(small_model)
)

p2 <- ggplot(small_model_influence, aes(x = Leverage, y = Residuals, size = Cooks_Distance)) +
  geom_point(alpha = 0.6, color = "green") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  scale_size_continuous(name = "Cook's Distance") +
  labs(title = "Residuals vs Leverage (Small Model)",
       x = "Leverage",
       y = "Residuals") +  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))
p1+p2
```
The Residuals vs Leverage plot for the Big Model shows a wide spread of residuals with varying leverage values, indicating potential influential poitns that could disproportionately impact the model's fit. Cook's Distance circles suggest the presence of points with moderate influence, though none appear to be extreme outliers. In contrast, the small model demonstrates a more clustered pattern of residuals and leverage, with minimal influential points, reflecting its simplicity and robustness against leverage but potentially lacking complexity to capture variability. 

## 9.4. Comparing the Models

```{r echo=TRUE, warning=FALSE, message=FALSE}
comparison <- data.frame(
  Model = c("Big Model", "Small Model"),
  R_squared = c(summary(big_model)$r.squared, summary(small_model)$r.squared),
  Adjusted_R_squared = c(summary(big_model)$adj.r.squared, summary(small_model)$adj.r.squared),
  AIC = c(AIC(big_model), AIC(small_model)),
  BIC = c(BIC(big_model), BIC(small_model))
)

kable(comparison, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)


```

### 9.4.1. Comparing R-sqaured and Adjusted-R-squared between big and small model

```{r echo=TRUE, warning=FALSE, message=FALSE, fig.height=6}

p1 <- ggplot(comparison, aes(Model, R_squared, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Model Comparison: R-squared", x = "Model", y = "R-squared") +  theme_classic(base_size = 8) +
   theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12)) +
  scale_fill_manual(values = c("Big Model" = "red", "Small Model" = "green"))

p2 <- ggplot(comparison, aes(Model, Adjusted_R_squared, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Model Comparison: Adjusted_R_squared", x = "Model", y = "Adjusted_R_squared") +  theme_classic(base_size = 8) +
    theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12)) +
  scale_fill_manual(values = c("Big Model" = "red", "Small Model" = "green"))

p1 +p2
```

Based on the Adjusted_R_squared, AIC, and BIC, the big_model showed a very good fit to the data given it showed the lowest most negative AIC, BIC values. This implies that together with other confounding factors, Gender better predicts an individual's BMI.Hence big demonstrated better performance when compared to small model on the training data set.

# 10. Model Validation

## 10.1. Calculating Prediction Errors

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Predictions
test_predictions_big <- predict(big_model, testing_data)
test_predictions_small <- predict(small_model, testing_data)

# Calculate errors
errors <- data.frame(
  Model = c("Big Model", "Small Model"),
  RMSPE = c(sqrt(mean((testing_data$BMI - test_predictions_big)^2)),
            sqrt(mean((testing_data$BMI - test_predictions_small)^2))),
  MAPE = c(mean(abs((testing_data$BMI - test_predictions_big) / testing_data$BMI)) * 100,mean(abs((testing_data$BMI - test_predictions_small) / testing_data$BMI)) * 100),
  R2 = c(cor(testing_data$BMI, test_predictions_big)^2,
         cor(testing_data$BMI, test_predictions_small)^2)
)
kable(errors, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

## 10.2. Visualizing the Predictions

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.height=6}
# Combine predictions and observed data
testing_data$Pred_Big <- test_predictions_big
testing_data$Pred_Small <- test_predictions_small

# Plot for Big Model
p1 <- ggplot(testing_data, aes(x = Pred_Big, y = BMI, color = "red")) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs Observed: Big Model",
       x = "Predicted BMI (Big Model)",
       y = "Observed BMI") +  theme_classic(base_size = 8) +
 theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))

# Plot for Small Model
p2 <- ggplot(testing_data, aes(x = Pred_Small, y = BMI)) +
  geom_point(color = "green") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs Observed: Small Model",
       x = "Predicted BMI (Small Model)",
       y = "Observed BMI") +  theme_classic(base_size = 8) +
  theme(
        axis.text = element_text(face = "bold",size = 10),
        axis.title.y = element_text(face = "bold", size = 10),
        axis.title.x = element_text(face = "bold", size = 10),
        plot.title = element_text(face = "bold", hjust = 0.5, size = 12))
p1+p2

```
The Predicted vs. Observed plots reveal that the Big Model has a wider spread of predicted values relative to the observed BMI, indicating that it captures some variability though not with high precision, as evidenced by the clustering at lower BMI values. The small model on the other hand showed a more constrained range of predicted values, tightly clustered around a narrow band hence failing to account for the variability in BMI. Although, the big model provided a slightly better predictions on the testing data, both models inadequately capture the variation in the data. 

## 10.3. Summarizing the Errors

```{r}
# residuals for both models
residuals_big <- testing_data$BMI - test_predictions_big
residuals_small <- testing_data$BMI - test_predictions_small

# Mean Absolute Error (MAE)
mae_big <- mean(abs(residuals_big))
mae_small <- mean(abs(residuals_small))

# Root Mean Squared Error (RMSE)
rmse_big <- sqrt(mean(residuals_big^2))
rmse_small <- sqrt(mean(residuals_small^2))

# R-squared for predictions
sst <- sum((testing_data$BMI - mean(testing_data$BMI))^2)  # Total Sum of Squares
sse_big <- sum(residuals_big^2)  # Sum of Squared Errors for big model
sse_small <- sum(residuals_small^2)  # Sum of Squared Errors for small model

r_squared_big <- 1 - (sse_big / sst)
r_squared_small <- 1 - (sse_small / sst)

# Create a summary table
comparison_table <- data.frame(
  Model = c("Big Model", "Small Model"),
  MAE = c(mae_big, mae_small),
  RMSE = c(rmse_big, rmse_small),
  R_Squared = c(r_squared_big, r_squared_small)
)

comparison_table %>%kable(digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

We observed that both models exhibited nearly indentical predictive performance, as reflected by their RMSPE values (28.017 vs. 28.020) and MAE values (26.999 vs. 26.99) and MAPE (88.274 vs. 88.260), indicating that the additional predictors in the Big do not significantly reduce prediction error. Whereas the big model shows a slightly higher $R^2$ (0.064 vs. 0.009), this improvement is minimal, and both models explain very little variance in the BMI. Conclusively, the results suggest that neither model effectively captures the variability in the outcome, with limited gains from including additional predictors in the big model.

## 10.4. Comparing the Models

Both the Big Model and Small Model exhibit similar RMSPE and MAPE values, indicating that the additional predictors in the Big Model (e.g., age, race, smoking status) do not significantly enhance prediction accuracy compared to the simpler Small Model. While the Big Model explains slightly more variance in BMI ($𝑅^2$=0.064) than the Small Model ($𝑅^2$=0.009), the low $𝑅^2$ values for both models suggest limited explanatory power and potential unmeasured confounding variables. The Big Model's inclusion of multiple predictors introduces complexity without meaningful gains in performance, hence resulting in potential overfitting. On the other hand, the Small Model's simplicity and comparable prediction errors make it a more practical and computationally efficient choice. \# 11. Discussion

## 11.1. Chosen Model

Based on these results, the Small Model is preferred, though further refinement of the Big Model with more relevant predictors may enhance its utility.

## 11.2. Answering My Question

The research question sought to determine whether gender is significantly associated with Body Mass Index (BMI), accounting for potential confounders such as age, race, and smoking status. The results from the Big Model, which included these additional predictors, showed a slightly higher $R^2$(0.064) compared to the Small Model ($R^2$ =0.009). However, both models exhibited low explanatory power overall, suggesting that gender alone, or even in combination with the included covariates, does not account for much of the variability in BMI. These findings partially align with pre-analysis expectations, as the additional predictors were anticipated to improve explanatory power, but the improvement was minimal.

## 11.3. Next Steps

The low $R^2$ values observed indicate that important predictors of BMI may be missing from the models. Factors such as dietary habits, physical activity, genetic predispositions, or socioeconomic status could contribute to the unexplained variance. Moreover, the observed similarity in prediction errors (RMSPE and MAPE) between the two models highlights potential limitations in the dataset or modeling approach. Future work should aim to include a broader range of predictors to enhance the models' ability to explain BMI variability

## 11.4. Reflection

This analysis provided me with great insights into the relationship between BMI and gender, as well as the impact of other confounding predictors like age, race/ethnicity, and smoking status. Despite thee big model's inclusion of multiple variables, its predictive performance was approximately identical to the simpler small model, highlighting the importance of parsimony in modeling. Low $R^2$ values across both models suggested unmeasured factors influencing BMI, while diagnostic checks revealed violation of model assumptions, emphasizing the need for model refinement. Ultimately, this process underscored the importance of validation, thoughtful model selection, and a potential for future model improvements through expanded datasets. \# 12. Session Information

```{r echo=TRUE, warning=FALSE, message=FALSE}
session_info()

```
